{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CGAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMcfhrnytwQBMHx6WTHGIx1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanucsengg/GAN/blob/main/CGAN_on_ciphar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOS1-f4TGek6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OQP__qoGhfE"
      },
      "source": [
        "from keras.datasets import cifar10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9BY6VxsGhmV",
        "outputId": "5d99c627-947e-479b-c7e1-6acd0e339c25"
      },
      "source": [
        "\n",
        "(x1, y1), (x2, y2) = cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "170508288/170498071 [==============================] - 6s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV3dIb7kGhpE"
      },
      "source": [
        "x = np.concatenate((x1, x2), axis=0)\n",
        "y = np.concatenate((y1, y2), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXw0btvrGvvR"
      },
      "source": [
        "\n",
        "del x1, y1, x2, y2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNigeQ8dISa9",
        "outputId": "2272aebc-ea64-4853-d2d7-7e2446ea1f43"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFVNeC2VIUqH",
        "outputId": "8e88122e-3204-4c40-f223-e40ce3af1c9f"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYBThlGbG14A",
        "outputId": "eaafc0b8-b3ba-4513-b753-9d062ca445f1"
      },
      "source": [
        "pip install scipy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okrRpc6OGw7e"
      },
      "source": [
        "import time\n",
        "from scipy import misc\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from keras.layers.advanced_activations import LeakyReLU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYrWkPcuGhsR"
      },
      "source": [
        "img_size = 32\n",
        "noise_size = 2048\n",
        "batch_size = 50\n",
        "classes = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1xDmhlKHmNG"
      },
      "source": [
        "def generator():\n",
        "    \n",
        "    noise = Input(shape=(noise_size, ))\n",
        "    label = Input(shape=(1, ))\n",
        "    \n",
        "    label_embedding = Flatten()(Embedding(classes, noise_size)(label))\n",
        "    \n",
        "    model_input = multiply([noise, label_embedding])\n",
        "    \n",
        "    x = Dense(2048)(model_input)\n",
        "    \n",
        "    x = Reshape((2, 2, 512))(x)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = LeakyReLU(0.1)(x)\n",
        "    \n",
        "    x = Conv2DTranspose(256, (5, 5), padding='same', strides=2)(x)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = LeakyReLU(0.1)(x)\n",
        "    \n",
        "    x = Conv2DTranspose(128, (5, 5), padding='same', strides=2)(x)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = LeakyReLU(0.1)(x)\n",
        "    \n",
        "    x = Conv2DTranspose(64, (5, 5), padding='same', strides=2)(x)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = LeakyReLU(0.1)(x)\n",
        "    \n",
        "    x = Conv2DTranspose(3, (5, 5), padding='same', strides=2)(x)\n",
        "    img = Activation('tanh')(x)\n",
        "    \n",
        "    return Model([noise, label], img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI6ByQ0YHqJK"
      },
      "source": [
        "def discriminator():\n",
        "    \n",
        "    img = Input(shape=(img_size, img_size, 3))\n",
        "    \n",
        "    x = GaussianNoise(0.1)(img)\n",
        "    \n",
        "    x = Conv2D(64, (3, 3), padding='same', strides = 2)(x)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "        \n",
        "    x = Conv2D(128, (3, 3), padding='same', strides = 2)(x)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "        \n",
        "    x = Conv2D(256, (3, 3), padding='same', strides = 2)(x)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "        \n",
        "    x = Conv2D(512, (3, 3), padding='same', strides = 2)(x)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    \n",
        "    label = Input(shape=(1, ))\n",
        "    label_embedding = Flatten()(Embedding(classes, noise_size)(label))\n",
        "    \n",
        "    flat_img = Flatten()(x)\n",
        "    \n",
        "    model_input = multiply([flat_img, label_embedding])\n",
        "\n",
        "    nn = Dropout(0.3)(model_input)\n",
        "    \n",
        "    validity = Dense(1, activation='sigmoid')(nn)\n",
        "    \n",
        "    return Model([img, label], validity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPFLw3g3HuPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f1676c-8e06-4e7b-f74d-b782f46a5424"
      },
      "source": [
        "\n",
        "d_model = discriminator()\n",
        "d_model.compile(loss=['binary_crossentropy'], optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "d_model.trainable = False\n",
        "g_model = generator()\n",
        "\n",
        "noise = Input(shape=(noise_size, ))\n",
        "label = Input(shape=(1, ))\n",
        "img = g_model([noise, label])\n",
        "\n",
        "valid = d_model([img, label])\n",
        "\n",
        "combined = Model([noise, label], valid)\n",
        "combined.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=0.001, beta_1=0.5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spsoKx6CHxuF"
      },
      "source": [
        "def train(epochs):\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        random = np.random.randint(0, 11)\n",
        "        \n",
        "        for index in range(int(x.shape[0]/batch_size)):\n",
        "                     \n",
        "            valid = np.ones((batch_size, 1)) - (np.random.random()*0.1)\n",
        "            fake = np.zeros((batch_size, 1)) + (np.random.random()*0.1)\n",
        "            \n",
        "            x_train = x[index*batch_size : (index+1)*batch_size]\n",
        "            y_train = y[index*batch_size : (index+1)*batch_size]\n",
        "            x_train = (x_train - 127.5)/127.5\n",
        "            \n",
        "            if index % 100 == random:\n",
        "                valid = np.zeros((batch_size, 1)) + (np.random.random()*0.1)\n",
        "                fake = np.ones((batch_size, 1)) - (np.random.random()*0.1)\n",
        "            \n",
        "            noise = np.random.randn(batch_size, noise_size)\n",
        "            gen_img = g_model.predict([noise, y_train])\n",
        "                        \n",
        "            d_loss_real = d_model.train_on_batch([x_train, y_train], valid)\n",
        "            d_loss_fake = d_model.train_on_batch([gen_img, y_train], fake)\n",
        "            d_loss = 0.5*(np.add(d_loss_real, d_loss_fake))\n",
        "\n",
        "            sample_label = np.random.randint(0, 10, batch_size).reshape(-1, 1)\n",
        "\n",
        "            valid = np.ones((batch_size, 1))\n",
        "            \n",
        "            g_loss = combined.train_on_batch([noise, sample_label], valid)\n",
        "\n",
        "            if index % (batch_size) == 0:\n",
        "                print(index)\n",
        "                print(\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss, g_loss))\n",
        "                sample_images(epoch)\n",
        "        \n",
        "        name = '/content/sample_data/weights/combined_' + str(epoch) + '.h5'\n",
        "        combined.save_weights(name)\n",
        "        \n",
        "        time.sleep(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a5XlY9BMDe4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2zC39KdH2GV"
      },
      "source": [
        "def sample_images(epoch):\n",
        "    r = 2\n",
        "    c = 5\n",
        "    noise = np.random.randn(10, noise_size)\n",
        "    sample_label = np.arange(0, 10).reshape(-1, 1)\n",
        "            \n",
        "    gen_img = g_model.predict([noise, sample_label])\n",
        "        \n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            img = image.array_to_img(gen_img[cnt])\n",
        "            axs[i,j].imshow(img)\n",
        "            axs[i,j].set_title(\"Class: %d\" % sample_label[cnt])\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"/content/sample_data/images/%d.png\" % epoch)\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK-hsA6mH5cA",
        "outputId": "23de0d09-8f73-4166-bc84-e32c719ef2b3"
      },
      "source": [
        "train(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0 [D loss: 0.660042] [G loss: 0.653806]\n",
            "50\n",
            "0 [D loss: 0.405845] [G loss: 0.427271]\n",
            "100\n",
            "0 [D loss: 0.260742] [G loss: 0.098891]\n",
            "150\n",
            "0 [D loss: 0.172595] [G loss: 0.049318]\n",
            "200\n",
            "0 [D loss: 0.183753] [G loss: 0.083055]\n",
            "250\n",
            "0 [D loss: 0.203654] [G loss: 0.047508]\n",
            "300\n",
            "0 [D loss: 0.188147] [G loss: 0.031626]\n",
            "350\n",
            "0 [D loss: 0.129009] [G loss: 0.038133]\n",
            "400\n",
            "0 [D loss: 0.194516] [G loss: 0.022915]\n",
            "450\n",
            "0 [D loss: 0.062565] [G loss: 0.031010]\n",
            "500\n",
            "0 [D loss: 0.297264] [G loss: 0.029245]\n",
            "550\n",
            "0 [D loss: 0.201348] [G loss: 0.030018]\n",
            "600\n",
            "0 [D loss: 0.219035] [G loss: 0.028703]\n",
            "650\n",
            "0 [D loss: 0.199625] [G loss: 0.038792]\n",
            "700\n",
            "0 [D loss: 0.142875] [G loss: 0.033770]\n",
            "750\n",
            "0 [D loss: 0.272500] [G loss: 0.060890]\n",
            "800\n",
            "0 [D loss: 0.210017] [G loss: 0.038706]\n",
            "850\n",
            "0 [D loss: 0.164209] [G loss: 0.069042]\n",
            "900\n",
            "0 [D loss: 0.209057] [G loss: 0.040888]\n",
            "950\n",
            "0 [D loss: 0.174913] [G loss: 0.055681]\n",
            "1000\n",
            "0 [D loss: 0.258968] [G loss: 0.031083]\n",
            "1050\n",
            "0 [D loss: 0.123873] [G loss: 0.088917]\n",
            "1100\n",
            "0 [D loss: 0.198831] [G loss: 0.053693]\n",
            "1150\n",
            "0 [D loss: 0.147070] [G loss: 0.060917]\n",
            "0\n",
            "1 [D loss: 0.215222] [G loss: 0.102809]\n",
            "50\n",
            "1 [D loss: 0.132268] [G loss: 0.178221]\n",
            "100\n",
            "1 [D loss: 0.187307] [G loss: 0.109203]\n",
            "150\n",
            "1 [D loss: 0.335313] [G loss: 0.092747]\n",
            "200\n",
            "1 [D loss: 0.227382] [G loss: 0.071547]\n",
            "250\n",
            "1 [D loss: 0.292238] [G loss: 0.130361]\n",
            "300\n",
            "1 [D loss: 0.118116] [G loss: 0.092889]\n",
            "350\n",
            "1 [D loss: 0.262871] [G loss: 0.100483]\n",
            "400\n",
            "1 [D loss: 0.208892] [G loss: 0.097014]\n",
            "450\n",
            "1 [D loss: 0.234153] [G loss: 0.113895]\n",
            "500\n",
            "1 [D loss: 0.115810] [G loss: 0.079870]\n",
            "550\n",
            "1 [D loss: 0.196481] [G loss: 0.137112]\n",
            "600\n",
            "1 [D loss: 0.255401] [G loss: 0.098687]\n",
            "650\n",
            "1 [D loss: 0.117794] [G loss: 0.123859]\n",
            "700\n",
            "1 [D loss: 0.205009] [G loss: 0.108416]\n",
            "750\n",
            "1 [D loss: 0.128782] [G loss: 0.128461]\n",
            "800\n",
            "1 [D loss: 0.083220] [G loss: 0.094538]\n",
            "850\n",
            "1 [D loss: 0.078128] [G loss: 0.112828]\n",
            "900\n",
            "1 [D loss: 0.097834] [G loss: 0.100430]\n",
            "950\n",
            "1 [D loss: 0.119971] [G loss: 0.109345]\n",
            "1000\n",
            "1 [D loss: 0.209830] [G loss: 0.103475]\n",
            "1050\n",
            "1 [D loss: 0.235915] [G loss: 0.101437]\n",
            "1100\n",
            "1 [D loss: 0.243788] [G loss: 0.094906]\n",
            "1150\n",
            "1 [D loss: 0.137069] [G loss: 0.113776]\n",
            "0\n",
            "2 [D loss: 2.676917] [G loss: 0.103060]\n",
            "50\n",
            "2 [D loss: 0.201478] [G loss: 0.067313]\n",
            "100\n",
            "2 [D loss: 2.817579] [G loss: 0.065669]\n",
            "150\n",
            "2 [D loss: 0.142391] [G loss: 0.173002]\n",
            "200\n",
            "2 [D loss: 2.825781] [G loss: 0.153551]\n",
            "250\n",
            "2 [D loss: 0.179114] [G loss: 0.158037]\n",
            "300\n",
            "2 [D loss: 2.843525] [G loss: 0.190708]\n",
            "350\n",
            "2 [D loss: 0.280162] [G loss: 0.149368]\n",
            "400\n",
            "2 [D loss: 2.842120] [G loss: 0.160506]\n",
            "450\n",
            "2 [D loss: 0.198211] [G loss: 0.150589]\n",
            "500\n",
            "2 [D loss: 2.751421] [G loss: 0.158108]\n",
            "550\n",
            "2 [D loss: 0.163757] [G loss: 0.116800]\n",
            "600\n",
            "2 [D loss: 2.908931] [G loss: 0.109090]\n",
            "650\n",
            "2 [D loss: 0.190501] [G loss: 0.112752]\n",
            "700\n",
            "2 [D loss: 2.722895] [G loss: 0.123304]\n",
            "750\n",
            "2 [D loss: 0.221935] [G loss: 0.123189]\n",
            "800\n",
            "2 [D loss: 2.895441] [G loss: 0.131151]\n",
            "850\n",
            "2 [D loss: 0.240117] [G loss: 0.119411]\n",
            "900\n",
            "2 [D loss: 2.620895] [G loss: 0.124457]\n",
            "950\n",
            "2 [D loss: 0.222937] [G loss: 0.109379]\n",
            "1000\n",
            "2 [D loss: 2.808712] [G loss: 0.121094]\n",
            "1050\n",
            "2 [D loss: 0.295010] [G loss: 0.120244]\n",
            "1100\n",
            "2 [D loss: 2.865664] [G loss: 0.163097]\n",
            "1150\n",
            "2 [D loss: 0.193521] [G loss: 0.116566]\n",
            "0\n",
            "3 [D loss: 0.259373] [G loss: 0.117620]\n",
            "50\n",
            "3 [D loss: 0.202575] [G loss: 0.126303]\n",
            "100\n",
            "3 [D loss: 0.125429] [G loss: 0.108029]\n",
            "150\n",
            "3 [D loss: 0.141161] [G loss: 0.103136]\n",
            "200\n",
            "3 [D loss: 0.136967] [G loss: 0.081439]\n",
            "250\n",
            "3 [D loss: 0.125890] [G loss: 0.103014]\n",
            "300\n",
            "3 [D loss: 0.106241] [G loss: 0.093661]\n",
            "350\n",
            "3 [D loss: 0.225554] [G loss: 0.115344]\n",
            "400\n",
            "3 [D loss: 0.244296] [G loss: 0.089461]\n",
            "450\n",
            "3 [D loss: 0.220823] [G loss: 0.122272]\n",
            "500\n",
            "3 [D loss: 0.301625] [G loss: 0.098918]\n",
            "550\n",
            "3 [D loss: 0.272097] [G loss: 0.118622]\n",
            "600\n",
            "3 [D loss: 0.259486] [G loss: 0.098742]\n",
            "650\n",
            "3 [D loss: 0.275886] [G loss: 0.110387]\n",
            "700\n",
            "3 [D loss: 0.251073] [G loss: 0.097379]\n",
            "750\n",
            "3 [D loss: 0.211663] [G loss: 0.125636]\n",
            "800\n",
            "3 [D loss: 0.151689] [G loss: 0.112920]\n",
            "850\n",
            "3 [D loss: 0.252123] [G loss: 0.151479]\n",
            "900\n",
            "3 [D loss: 0.197670] [G loss: 0.118262]\n",
            "950\n",
            "3 [D loss: 0.221885] [G loss: 0.125023]\n",
            "1000\n",
            "3 [D loss: 0.221052] [G loss: 0.115517]\n",
            "1050\n",
            "3 [D loss: 0.124774] [G loss: 0.122930]\n",
            "1100\n",
            "3 [D loss: 0.203910] [G loss: 0.118945]\n",
            "1150\n",
            "3 [D loss: 0.134955] [G loss: 0.125634]\n",
            "0\n",
            "4 [D loss: 0.193069] [G loss: 0.103139]\n",
            "50\n",
            "4 [D loss: 0.259109] [G loss: 0.124094]\n",
            "100\n",
            "4 [D loss: 0.195472] [G loss: 0.103844]\n",
            "150\n",
            "4 [D loss: 0.225093] [G loss: 0.133297]\n",
            "200\n",
            "4 [D loss: 0.163918] [G loss: 0.107704]\n",
            "250\n",
            "4 [D loss: 0.101814] [G loss: 0.144739]\n",
            "300\n",
            "4 [D loss: 0.189591] [G loss: 0.113411]\n",
            "350\n",
            "4 [D loss: 0.293867] [G loss: 0.123768]\n",
            "400\n",
            "4 [D loss: 0.173012] [G loss: 0.113851]\n",
            "450\n",
            "4 [D loss: 0.221858] [G loss: 0.138618]\n",
            "500\n",
            "4 [D loss: 0.172942] [G loss: 0.116009]\n",
            "550\n",
            "4 [D loss: 0.136871] [G loss: 0.117590]\n",
            "600\n",
            "4 [D loss: 0.237273] [G loss: 0.096190]\n",
            "650\n",
            "4 [D loss: 0.186274] [G loss: 0.147027]\n",
            "700\n",
            "4 [D loss: 0.255245] [G loss: 0.125262]\n",
            "750\n",
            "4 [D loss: 0.249754] [G loss: 0.165407]\n",
            "800\n",
            "4 [D loss: 0.277328] [G loss: 0.107246]\n",
            "850\n",
            "4 [D loss: 0.142585] [G loss: 0.142867]\n",
            "900\n",
            "4 [D loss: 0.102641] [G loss: 0.120962]\n",
            "950\n",
            "4 [D loss: 0.271273] [G loss: 0.130809]\n",
            "1000\n",
            "4 [D loss: 0.109658] [G loss: 0.106395]\n",
            "1050\n",
            "4 [D loss: 0.261783] [G loss: 0.130733]\n",
            "1100\n",
            "4 [D loss: 0.131221] [G loss: 0.112224]\n",
            "1150\n",
            "4 [D loss: 0.161472] [G loss: 0.140246]\n",
            "0\n",
            "5 [D loss: 0.244638] [G loss: 0.120552]\n",
            "50\n",
            "5 [D loss: 0.196348] [G loss: 0.122789]\n",
            "100\n",
            "5 [D loss: 0.166144] [G loss: 0.109819]\n",
            "150\n",
            "5 [D loss: 0.176522] [G loss: 0.123031]\n",
            "200\n",
            "5 [D loss: 0.142700] [G loss: 0.109373]\n",
            "250\n",
            "5 [D loss: 0.269337] [G loss: 0.156404]\n",
            "300\n",
            "5 [D loss: 0.254345] [G loss: 0.131696]\n",
            "350\n",
            "5 [D loss: 0.164532] [G loss: 0.159065]\n",
            "400\n",
            "5 [D loss: 0.106116] [G loss: 0.118803]\n",
            "450\n",
            "5 [D loss: 0.277169] [G loss: 0.134482]\n",
            "500\n",
            "5 [D loss: 0.287866] [G loss: 0.104973]\n",
            "550\n",
            "5 [D loss: 0.248589] [G loss: 0.123104]\n",
            "600\n",
            "5 [D loss: 0.295720] [G loss: 0.103040]\n",
            "650\n",
            "5 [D loss: 0.100914] [G loss: 0.140377]\n",
            "700\n",
            "5 [D loss: 0.163738] [G loss: 0.137215]\n",
            "750\n",
            "5 [D loss: 0.171772] [G loss: 0.128703]\n",
            "800\n",
            "5 [D loss: 0.260751] [G loss: 0.118659]\n",
            "850\n",
            "5 [D loss: 0.132563] [G loss: 0.158176]\n",
            "900\n",
            "5 [D loss: 0.068948] [G loss: 0.131109]\n",
            "950\n",
            "5 [D loss: 0.176061] [G loss: 0.133980]\n",
            "1000\n",
            "5 [D loss: 0.258799] [G loss: 0.122497]\n",
            "1050\n",
            "5 [D loss: 0.276342] [G loss: 0.129235]\n",
            "1100\n",
            "5 [D loss: 0.182110] [G loss: 0.108329]\n",
            "1150\n",
            "5 [D loss: 0.205929] [G loss: 0.123243]\n",
            "0\n",
            "6 [D loss: 0.239501] [G loss: 0.119042]\n",
            "50\n",
            "6 [D loss: 0.146407] [G loss: 0.137013]\n",
            "100\n",
            "6 [D loss: 0.265129] [G loss: 0.120788]\n",
            "150\n",
            "6 [D loss: 0.188882] [G loss: 0.128230]\n",
            "200\n",
            "6 [D loss: 0.105036] [G loss: 0.111474]\n",
            "250\n",
            "6 [D loss: 0.291795] [G loss: 0.136637]\n",
            "300\n",
            "6 [D loss: 0.239091] [G loss: 0.109948]\n",
            "350\n",
            "6 [D loss: 0.279201] [G loss: 0.224390]\n",
            "400\n",
            "6 [D loss: 0.244165] [G loss: 0.179253]\n",
            "450\n",
            "6 [D loss: 0.196471] [G loss: 0.232439]\n",
            "500\n",
            "6 [D loss: 0.183000] [G loss: 0.196017]\n",
            "550\n",
            "6 [D loss: 0.147866] [G loss: 0.249822]\n",
            "600\n",
            "6 [D loss: 0.107636] [G loss: 0.198266]\n",
            "650\n",
            "6 [D loss: 0.233946] [G loss: 0.222648]\n",
            "700\n",
            "6 [D loss: 0.310137] [G loss: 0.202066]\n",
            "750\n",
            "6 [D loss: 0.322743] [G loss: 0.212722]\n",
            "800\n",
            "6 [D loss: 0.209874] [G loss: 0.185308]\n",
            "850\n",
            "6 [D loss: 0.087023] [G loss: 0.157439]\n",
            "900\n",
            "6 [D loss: 0.086420] [G loss: 0.142396]\n",
            "950\n",
            "6 [D loss: 0.211256] [G loss: 0.150144]\n",
            "1000\n",
            "6 [D loss: 0.230216] [G loss: 0.144865]\n",
            "1050\n",
            "6 [D loss: 0.274272] [G loss: 0.175506]\n",
            "1100\n",
            "6 [D loss: 0.187175] [G loss: 0.151865]\n",
            "1150\n",
            "6 [D loss: 0.241845] [G loss: 0.145131]\n",
            "0\n",
            "7 [D loss: 0.228916] [G loss: 0.133198]\n",
            "50\n",
            "7 [D loss: 0.288379] [G loss: 0.163780]\n",
            "100\n",
            "7 [D loss: 0.253715] [G loss: 0.156303]\n",
            "150\n",
            "7 [D loss: 0.232227] [G loss: 0.149393]\n",
            "200\n",
            "7 [D loss: 0.185525] [G loss: 0.133110]\n",
            "250\n",
            "7 [D loss: 0.281131] [G loss: 0.117158]\n",
            "300\n",
            "7 [D loss: 0.183833] [G loss: 0.113488]\n",
            "350\n",
            "7 [D loss: 0.098922] [G loss: 0.130024]\n",
            "400\n",
            "7 [D loss: 0.239431] [G loss: 0.120745]\n",
            "450\n",
            "7 [D loss: 0.224550] [G loss: 0.120615]\n",
            "500\n",
            "7 [D loss: 0.132888] [G loss: 0.106021]\n",
            "550\n",
            "7 [D loss: 0.213298] [G loss: 0.136567]\n",
            "600\n",
            "7 [D loss: 0.205435] [G loss: 0.102629]\n",
            "650\n",
            "7 [D loss: 0.198097] [G loss: 0.129180]\n",
            "700\n",
            "7 [D loss: 0.165581] [G loss: 0.108853]\n",
            "750\n",
            "7 [D loss: 0.209665] [G loss: 0.130661]\n",
            "800\n",
            "7 [D loss: 0.108829] [G loss: 0.111200]\n",
            "850\n",
            "7 [D loss: 0.174158] [G loss: 0.131067]\n",
            "900\n",
            "7 [D loss: 0.088524] [G loss: 0.111886]\n",
            "950\n",
            "7 [D loss: 0.237800] [G loss: 0.110132]\n",
            "1000\n",
            "7 [D loss: 0.252993] [G loss: 0.094379]\n",
            "1050\n",
            "7 [D loss: 0.218194] [G loss: 0.117996]\n",
            "1100\n",
            "7 [D loss: 0.232045] [G loss: 0.106538]\n",
            "1150\n",
            "7 [D loss: 0.212984] [G loss: 0.120009]\n",
            "0\n",
            "8 [D loss: 0.202061] [G loss: 0.110582]\n",
            "50\n",
            "8 [D loss: 0.179828] [G loss: 0.111934]\n",
            "100\n",
            "8 [D loss: 0.172411] [G loss: 0.097763]\n",
            "150\n",
            "8 [D loss: 0.256087] [G loss: 0.113659]\n",
            "200\n",
            "8 [D loss: 0.202475] [G loss: 0.103046]\n",
            "250\n",
            "8 [D loss: 0.066595] [G loss: 0.110737]\n",
            "300\n",
            "8 [D loss: 0.200186] [G loss: 0.090991]\n",
            "350\n",
            "8 [D loss: 0.188816] [G loss: 0.113127]\n",
            "400\n",
            "8 [D loss: 0.146583] [G loss: 0.104058]\n",
            "450\n",
            "8 [D loss: 0.209924] [G loss: 0.152225]\n",
            "500\n",
            "8 [D loss: 0.181156] [G loss: 0.125904]\n",
            "550\n",
            "8 [D loss: 0.072390] [G loss: 0.169101]\n",
            "600\n",
            "8 [D loss: 0.123518] [G loss: 0.136363]\n",
            "650\n",
            "8 [D loss: 0.310744] [G loss: 0.142682]\n",
            "700\n",
            "8 [D loss: 0.226361] [G loss: 0.131594]\n",
            "750\n",
            "8 [D loss: 0.302425] [G loss: 0.141387]\n",
            "800\n",
            "8 [D loss: 0.219856] [G loss: 0.126485]\n",
            "850\n",
            "8 [D loss: 0.234461] [G loss: 0.141394]\n",
            "900\n",
            "8 [D loss: 0.191021] [G loss: 0.127263]\n",
            "950\n",
            "8 [D loss: 0.084763] [G loss: 0.139661]\n",
            "1000\n",
            "8 [D loss: 0.113127] [G loss: 0.123112]\n",
            "1050\n",
            "8 [D loss: 0.205938] [G loss: 0.123475]\n",
            "1100\n",
            "8 [D loss: 0.241492] [G loss: 0.109355]\n",
            "1150\n",
            "8 [D loss: 0.213417] [G loss: 0.128050]\n",
            "0\n",
            "9 [D loss: 0.171342] [G loss: 0.110752]\n",
            "50\n",
            "9 [D loss: 0.186746] [G loss: 0.124141]\n",
            "100\n",
            "9 [D loss: 0.314470] [G loss: 0.109635]\n",
            "150\n",
            "9 [D loss: 0.135608] [G loss: 0.117444]\n",
            "200\n",
            "9 [D loss: 0.208676] [G loss: 0.099384]\n",
            "250\n",
            "9 [D loss: 0.172456] [G loss: 0.105548]\n",
            "300\n",
            "9 [D loss: 0.148594] [G loss: 0.102729]\n",
            "350\n",
            "9 [D loss: 0.154227] [G loss: 0.118474]\n",
            "400\n",
            "9 [D loss: 0.248129] [G loss: 0.102938]\n",
            "450\n",
            "9 [D loss: 0.177967] [G loss: 0.124101]\n",
            "500\n",
            "9 [D loss: 0.133596] [G loss: 0.106245]\n",
            "550\n",
            "9 [D loss: 0.100076] [G loss: 0.106681]\n",
            "600\n",
            "9 [D loss: 0.183943] [G loss: 0.102910]\n",
            "650\n",
            "9 [D loss: 0.245494] [G loss: 0.111023]\n",
            "700\n",
            "9 [D loss: 0.276280] [G loss: 0.093209]\n",
            "750\n",
            "9 [D loss: 0.199065] [G loss: 0.108466]\n",
            "800\n",
            "9 [D loss: 0.155602] [G loss: 0.099411]\n",
            "850\n",
            "9 [D loss: 0.310576] [G loss: 0.133241]\n",
            "900\n",
            "9 [D loss: 0.198796] [G loss: 0.115863]\n",
            "950\n",
            "9 [D loss: 0.094920] [G loss: 0.113141]\n",
            "1000\n",
            "9 [D loss: 0.129147] [G loss: 0.101359]\n",
            "1050\n",
            "9 [D loss: 0.292424] [G loss: 0.115158]\n",
            "1100\n",
            "9 [D loss: 0.156998] [G loss: 0.113531]\n",
            "1150\n",
            "9 [D loss: 0.198736] [G loss: 0.129405]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlAcAKuhH-bN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b348eb-caeb-425a-c338-b31407cc5ad3"
      },
      "source": [
        "g_model.save('/content/sample_data/gan-model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: /content/sample_data/gan-model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNW7JrkV8pQh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_D8m9sMGiED"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn0YYZhL8qAs"
      },
      "source": [
        "pickle.dump({'train': train_history, 'test': test_history},\n",
        "            open('../logs/acgan-history.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kivkhnJGiG_"
      },
      "source": [
        "hist = pickle.load(open('../logs/acgan-history.pkl'))\n",
        "\n",
        "for p in ['train', 'test']:\n",
        "    for g in ['discriminator', 'generator']:\n",
        "        hist[p][g] = pd.DataFrame(hist[p][g], columns=['loss', 'generation_loss', 'auxiliary_loss'])\n",
        "        plt.plot(hist[p][g]['generation_loss'], label='{} ({})'.format(g, p))\n",
        "\n",
        "# get the NE and show as an equilibrium point\n",
        "plt.hlines(-np.log(0.5), 0, hist[p][g]['generation_loss'].shape[0], label='Nash Equilibrium')\n",
        "plt.legend()\n",
        "plt.title(r'$L_s$ (generation loss) per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel(r'$L_s$')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP7hQFJsGiJz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ptj51mjGiOf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRJ3kLXsGiSt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxIXmLQEGiUg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV3SnnLiGiWA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}